version: 2

defaults: &defaults
  docker:
    - image: palantirtechnologies/circle-spark
  resource_class: xlarge
  environment:
    TERM: dumb

jobs:
  build:
    <<: *defaults
    steps:
      # Saves us from recompiling every time...
      - restore_cache:
          keys:
            - whole-build-{{ .Branch }}-{{ .BuildNum }}
            - whole-build-{{ .Branch }}-
            - whole-build-master-
      - checkout
      # Given the whole-build cache, this is superfluous, but leave it in in case we will want to remove the former
      - restore_cache:
          keys:
            - maven-dependency-cache-{{ checksum "pom.xml" }}
            # Fallback - see https://circleci.com/docs/2.0/configuration-reference/#example-2
            - maven-dependency-cache-
      - restore_cache:
          keys:
            - build-binaries-{{ checksum "build/mvn" }}-{{ checksum "build/sbt" }}
            - build-binaries-
      - run: |
          set -o pipefail \
          && ./build/mvn -DskipTests -Phadoop-cloud -Phadoop-palantir -Pkinesis-asl -Pkubernetes -Pyarn -Phive -Psparkr install \
            | tee -a "/tmp/mvn-install.log"
      - store_artifacts:
          path: /tmp/mvn-install.log
          destination: command-logs
      # Get sbt to run trivially, ensures its launcher is downloaded under build/
      - run: ./build/sbt -h || true
      - save_cache:
          key: build-binaries-{{ checksum "build/mvn" }}-{{ checksum "build/sbt" }}
          paths:
            - "build"
      - save_cache:
          key: maven-dependency-cache-{{ checksum "pom.xml" }}
          paths:
            - "~/.m2"
      - persist_to_workspace:
          root: .
          paths:
            - "*/target/*"

      # SBT dependencies

      - restore_cache:
          keys:
            - ivy-dependency-cache-{{ checksum "pom.xml" }}
            - ivy-dependency-cache-
      - run: |
          # Make sbt fetch all the external deps to ~/.ivy2 so it gets cached
          set -o pipefail \
            && ./build/sbt -Phadoop-cloud -Phadoop-palantir -Pkinesis-asl -Pkubernetes -Pyarn -Phive -Psparkr externalDependencyClasspath \
              | tee -a "/tmp/sbt-classpath.log"
      - store_artifacts:
          path: /tmp/sbt-classpath.log
          destination: command-logs
      - save_cache:
          key: ivy-dependency-cache-{{ checksum "pom.xml" }}
          paths:
            - "~/.ivy2"
      # And finally save the whole project directory
      - save_cache:
          key: whole-build-{{ .Branch }}-{{ .BuildNum }}
          paths: .

  test:
    <<: *defaults
    # project/CirclePlugin.scala does its own test splitting in SBT based on CIRCLE_NODE_INDEX, CIRCLE_NODE_TOTAL
    parallelism: 6
    # Spark runs a lot of tests in parallel, we need 16 GB of RAM for this
    resource_class: xlarge
    environment:
      CIRCLE_TEST_REPORTS: /tmp/circle-test-reports
    steps:
      - checkout
      - attach_workspace:
          at: .
      - restore_cache:
          key: maven-dependency-cache-{{ checksum "pom.xml" }}
      - restore_cache:
          key: ivy-dependency-cache-{{ checksum "pom.xml" }}
      - restore_cache:
          key: build-binaries-{{ checksum "build/mvn" }}-{{ checksum "build/sbt" }}
      - run:
          name: Before running tests, ensure we created the CIRCLE_TEST_REPORTS directory
          command: mkdir -p $CIRCLE_TEST_REPORTS
      - run:
          name: Run all tests
          command: set -o pipefail && HADOOP_PROFILE=hadooppalantir ./dev/run-tests | tee -a "/tmp/run-tests.log"
      - store_artifacts:
          path: /tmp/run-tests.log
          destination: command-logs
      - run:
          name: Collect unit tests
          command: mkdir -p /tmp/unit-tests && find . -name unit-tests.log -exec rsync -R {} /tmp/unit-tests/ \;
          when: always
      - store_artifacts:
          path: /tmp/unit-tests
      - store_test_results:
          # TODO(dsanduleac): can we use $CIRCLE_TEST_RESULTS here?
          path: /tmp/circle-test-reports
      - run:
          name: Collect yarn integration test logs
          command: |
            shopt -s nullglob
            files=(resource-managers/yarn/target/./org.apache.spark.deploy.yarn.*/*-logDir-*)
            mkdir -p /tmp/yarn-tests
            if [[ ${#files[@]} != 0 ]]; then
              rsync -Rrm "${files[@]}" /tmp/yarn-tests/
            fi
          when: always
      - store_artifacts:
          path: /tmp/yarn-tests
  deploy:
    <<: *defaults
    steps:
      - checkout
      - attach_workspace:
          at: .
      - restore_cache:
          key: maven-dependency-cache-{{ checksum "pom.xml" }}
      - restore_cache:
          key: build-binaries-{{ checksum "build/mvn" }}-{{ checksum "build/sbt" }}
      - run: echo "user=$BINTRAY_USERNAME" > .credentials
      - run: echo "password=$BINTRAY_PASSWORD" >> .credentials
      - run: echo "realm=Bintray API Realm" >> .credentials
      - run: echo "host=api.bintray.com" >> .credentials
      - deploy: dev/publish.sh
      - store_artifacts:
          path: /tmp/make-distribution.log
          destination: command-logs
      - store_artifacts:
          path: /tmp/publish_artifacts.log
          destination: command-logs
      - deploy: curl -u $BINTRAY_USERNAME:$BINTRAY_PASSWORD -X POST https://api.bintray.com/content/palantir/releases/spark/$(git describe --tags)/publish

workflows:
  version: 2
  build-test-deploy:
    jobs:
      - build:
          filters:
            # run on all branches and tags
            tags:
              only: /.*/
      - test:
          requires:
            - build
          filters:
            # run on all branches and tags
            tags:
              only: /.*/
      - deploy:
          requires:
            - build
            - test
          filters:
            tags:
              only: /[0-9]+(?:\.[0-9]+){2,}-palantir\.[0-9]+(?:\.[0-9]+)*/
            branches:
              only: master
