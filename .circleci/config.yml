version: 2

defaults: &defaults
  docker:
    - image: palantirtechnologies/circle-spark
  resource_class: xlarge

jobs:
  build:
    <<: *defaults
    steps:
      - checkout
      - run: echo "user=$BINTRAY_USERNAME" > .credentials
      - run: echo "password=$BINTRAY_PASSWORD" >> .credentials
      - run: echo "realm=Bintray API Realm" >> .credentials
      - run: echo "host=api.bintray.com" >> .credentials
      - restore_cache:
          key: dependency-cache-{{ checksum "pom.xml" }}
      - run: |
          set -o pipefail \
          && ./build/mvn -DskipTests -Phadoop-cloud -Phadoop-palantir -Pkinesis-asl -Pkubernetes -Pyarn -Phive -Psparkr install \
            | tee -a "/tmp/mvn-install.log"
      - store_artifacts:
          path: /tmp/mvn-install.log
      - run: |
          # Make sbt fetch all the external deps to ~/.ivy2 so it gets cached
          set -o pipefail \
            && ./build/sbt -Phadoop-cloud -Phadoop-palantir -Pkinesis-asl -Pkubernetes -Pyarn -Phive -Psparkr externalDependencyClasspath \
              | tee -a "/tmp/sbt-classpath.log"
      - store_artifacts:
          path: /tmp/sbt-classpath.log
      - save_cache:
          key: dependency-cache-{{ checksum "pom.xml" }}
          paths:
            - "~/.ivy2"
            - "~/.m2"
      - persist_to_workspace:
          root: .
          paths:
            - "*/target/*"

    environment:
      - TERM: dumb

  test:
    <<: *defaults
    # project/CirclePlugin.scala does its own test splitting in SBT based on CIRCLE_NODE_INDEX, CIRCLE_NODE_TOTAL
    parallelism: 6
    # Spark runs a lot of tests in parallel, we need 16 GB of RAM for this
    resource_class: xlarge
    steps:
      - attach_workspace:
          at: .
      - restore_cache:
          key: dependency-cache-{{ checksum "pom.xml" }}
      - run: set -o pipefail && HADOOP_PROFILE=hadooppalantir ./dev/run-tests | tee -a "/tmp/run-tests.log"
      - store_artifacts:
          path: /tmp/run-tests.log
      - run:
          name: Collect unit tests
          command: mkdir -p /tmp/unit-tests && find . -name unit-tests.log -exec rsync -R {} /tmp/unit-tests/ \;
      - store_artifacts:
          path: /tmp/unit-tests
      - run:
          name: Collect yarn integration test logs
          command: |
            shopt -s nullglob
            files=(resource-managers/yarn/target/./org.apache.spark.deploy.yarn.*/*-logDir-*)
            mkdir -p /tmp/yarn-tests
            if [[ ${#files[@]} != 0 ]]; then
              rsync -Rrm "${files[@]}" /tmp/yarn-tests/
            fi
      - store_artifacts:
          path: /tmp/yarn-tests
  deploy:
    <<: *defaults
    steps:
      - run: dev/publish.sh
      - run: curl -u $BINTRAY_USERNAME:$BINTRAY_PASSWORD -X POST https://api.bintray.com/content/palantir/releases/spark/$(git describe --tags)/publish

workflows:
  version: 2
  build-test-deploy:
    jobs:
      - build:
          filters:
            # run on all branches and tags
            tags:
              only: /.*/
      - test:
          requires:
            - build
          filters:
            # run on all branches and tags
            tags:
              only: /.*/
      - deploy:
          requires:
            - build
            - test
          filters:
            tags:
              only: /[0-9]+(?:\.[0-9]+){2,}-palantir\.[0-9]+(?:\.[0-9]+)*/
            branches:
              only: master
