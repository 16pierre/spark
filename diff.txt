diff --cc core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala
index 2d2a3cc8db,9989f68f85..0000000000
--- a/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala
+++ b/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala
@@@ -347,12 -349,18 +350,27 @@@ private[spark] abstract class BasePytho
          Thread.sleep(2000)
        }
        if (!context.isCompleted) {
++<<<<<<< HEAD
 +        try {
 +          logWarning("Incomplete task interrupted: Attempting to kill Python Worker")
 +          env.destroyPythonWorker(pythonExec, envVars.asScala.toMap, condaInstructions, worker)
 +        } catch {
 +          case e: Exception =>
 +            logError("Exception when trying to kill worker", e)
++=======
+         Thread.sleep(taskKillTimeout)
+         if (!context.isCompleted) {
+           try {
+             // Mimic the task name used in `Executor` to help the user find out the task to blame.
+             val taskName = s"${context.partitionId}.${context.taskAttemptId} " +
+               s"in stage ${context.stageId} (TID ${context.taskAttemptId})"
+             logWarning(s"Incomplete task $taskName interrupted: Attempting to kill Python Worker")
+             env.destroyPythonWorker(pythonExec, envVars.asScala.toMap, worker)
+           } catch {
+             case e: Exception =>
+               logError("Exception when trying to kill worker", e)
+           }
++>>>>>>> origin/master
          }
        }
      }
diff --cc core/src/test/scala/org/apache/spark/metrics/MetricsSystemSuite.scala
index ea79a88c9c,a7a24114f1..0000000000
--- a/core/src/test/scala/org/apache/spark/metrics/MetricsSystemSuite.scala
+++ b/core/src/test/scala/org/apache/spark/metrics/MetricsSystemSuite.scala
@@@ -39,18 -42,22 +40,28 @@@ class MetricsSystemSuite extends SparkF
    test("MetricsSystem with default config") {
      val metricsSystem = MetricsSystem.createMetricsSystem("default", conf, securityMgr)
      metricsSystem.start()
++<<<<<<< HEAD
++=======
+     val sources = PrivateMethod[ArrayBuffer[Source]]('sources)
+     val sinks = PrivateMethod[ArrayBuffer[Sink]]('sinks)
++>>>>>>> origin/master
  
 -    assert(metricsSystem.invokePrivate(sources()).length === StaticSources.allSources.length)
 -    assert(metricsSystem.invokePrivate(sinks()).length === 0)
 +    assert(metricsSystem.getSources.length === StaticSources.allSources.length)
 +    assert(metricsSystem.getSinks.length === 0)
      assert(metricsSystem.getServletHandlers.nonEmpty)
    }
  
    test("MetricsSystem with sources add") {
      val metricsSystem = MetricsSystem.createMetricsSystem("test", conf, securityMgr)
      metricsSystem.start()
++<<<<<<< HEAD
++=======
+     val sources = PrivateMethod[ArrayBuffer[Source]]('sources)
+     val sinks = PrivateMethod[ArrayBuffer[Sink]]('sinks)
++>>>>>>> origin/master
  
 -    assert(metricsSystem.invokePrivate(sources()).length === StaticSources.allSources.length)
 -    assert(metricsSystem.invokePrivate(sinks()).length === 1)
 +    assert(metricsSystem.getSources.length === StaticSources.allSources.length)
 +    assert(metricsSystem.getSinks.length === 1)
      assert(metricsSystem.getServletHandlers.nonEmpty)
  
      val source = new MasterSource(null)
diff --cc pom.xml
index 99e1bd4d2f,0297311dd6..0000000000
--- a/pom.xml
+++ b/pom.xml
@@@ -112,9 -111,11 +112,15 @@@
      <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
      <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
      <java.version>1.8</java.version>
++<<<<<<< HEAD
 +    <maven.version>3.5.0</maven.version>
++=======
+     <maven.compiler.source>${java.version}</maven.compiler.source>
+     <maven.compiler.target>${java.version}</maven.compiler.target>
+     <maven.version>3.3.9</maven.version>
++>>>>>>> origin/master
      <sbt.project.name>spark</sbt.project.name>
 -    <slf4j.version>1.7.16</slf4j.version>
 +    <slf4j.version>1.7.25</slf4j.version>
      <log4j.version>1.2.17</log4j.version>
      <hadoop.version>2.6.5</hadoop.version>
      <protobuf.version>2.5.0</protobuf.version>
diff --cc sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
index 7605085274,3452a1e715..0000000000
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
@@@ -285,13 -284,24 +284,31 @@@ object SQLConf 
      .booleanConf
      .createWithDefault(true)
  
++<<<<<<< HEAD
 +  val PARQUET_TIMESTAMP_AS_INT96 = buildConf("spark.sql.parquet.timestampAsInt96")
 +    .doc("Write timestamps as int96 to maintain legacy compatibility")
 +    .booleanConf
 +    .createWithDefault(false)
++=======
+   object ParquetOutputTimestampType extends Enumeration {
+     val INT96, TIMESTAMP_MICROS, TIMESTAMP_MILLIS = Value
+   }
+ 
+   val PARQUET_OUTPUT_TIMESTAMP_TYPE = buildConf("spark.sql.parquet.outputTimestampType")
+     .doc("Sets which Parquet timestamp type to use when Spark writes data to Parquet files. " +
+       "INT96 is a non-standard but commonly used timestamp type in Parquet. TIMESTAMP_MICROS " +
+       "is a standard timestamp type in Parquet, which stores number of microseconds from the " +
+       "Unix epoch. TIMESTAMP_MILLIS is also standard, but with millisecond precision, which " +
+       "means Spark has to truncate the microsecond portion of its timestamp value.")
+     .stringConf
+     .transform(_.toUpperCase(Locale.ROOT))
+     .checkValues(ParquetOutputTimestampType.values.map(_.toString))
+     .createWithDefault(ParquetOutputTimestampType.INT96.toString)
++>>>>>>> origin/master
  
    val PARQUET_INT64_AS_TIMESTAMP_MILLIS = buildConf("spark.sql.parquet.int64AsTimestampMillis")
-     .doc("When true, timestamp values will be stored as INT64 with TIMESTAMP_MILLIS as the " +
+     .doc(s"(Deprecated since Spark 2.3, please set ${PARQUET_OUTPUT_TIMESTAMP_TYPE.key}.) " +
+       "When true, timestamp values will be stored as INT64 with TIMESTAMP_MILLIS as the " +
        "extended type. In this mode, the microsecond portion of the timestamp value will be" +
        "truncated.")
      .booleanConf
@@@ -1155,9 -1192,21 +1206,25 @@@ class SQLConf extends Serializable wit
  
    def isParquetINT64AsTimestampMillis: Boolean = getConf(PARQUET_INT64_AS_TIMESTAMP_MILLIS)
  
+   def parquetOutputTimestampType: ParquetOutputTimestampType.Value = {
+     val isOutputTimestampTypeSet = settings.containsKey(PARQUET_OUTPUT_TIMESTAMP_TYPE.key)
+     if (!isOutputTimestampTypeSet && isParquetINT64AsTimestampMillis) {
+       // If PARQUET_OUTPUT_TIMESTAMP_TYPE is not set and PARQUET_INT64_AS_TIMESTAMP_MILLIS is set,
+       // respect PARQUET_INT64_AS_TIMESTAMP_MILLIS and use TIMESTAMP_MILLIS. Otherwise,
+       // PARQUET_OUTPUT_TIMESTAMP_TYPE has higher priority.
+       ParquetOutputTimestampType.TIMESTAMP_MILLIS
+     } else {
+       ParquetOutputTimestampType.withName(getConf(PARQUET_OUTPUT_TIMESTAMP_TYPE))
+     }
+   }
+ 
    def writeLegacyParquetFormat: Boolean = getConf(PARQUET_WRITE_LEGACY_FORMAT)
  
++<<<<<<< HEAD
 +  def isParquetTimestampAsINT96: Boolean = getConf(PARQUET_TIMESTAMP_AS_INT96)
++=======
+   def parquetRecordFilterEnabled: Boolean = getConf(PARQUET_RECORD_FILTER_ENABLED)
++>>>>>>> origin/master
  
    def inMemoryPartitionPruning: Boolean = getConf(IN_MEMORY_PARTITION_PRUNING)
  
diff --cc sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java
index fb1fcd77b0,0f1f470dc5..0000000000
--- a/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java
+++ b/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java
@@@ -95,14 -92,16 +95,24 @@@ public class VectorizedColumnReader 
  
    private final PageReader pageReader;
    private final ColumnDescriptor descriptor;
++<<<<<<< HEAD
 +  private final Type fullType;
 +
 +  public VectorizedColumnReader(ColumnDescriptor descriptor, PageReader pageReader, Type fullType)
 +      throws IOException {
++=======
+   private final OriginalType originalType;
+ 
+   public VectorizedColumnReader(
+       ColumnDescriptor descriptor,
+       OriginalType originalType,
+       PageReader pageReader) throws IOException {
++>>>>>>> origin/master
      this.descriptor = descriptor;
      this.pageReader = pageReader;
+     this.originalType = originalType;
      this.maxDefLevel = descriptor.getMaxDefinitionLevel();
 +    this.fullType = fullType;
  
      DictionaryPage dictionaryPage = pageReader.readDictionaryPage();
      if (dictionaryPage != null) {
@@@ -168,8 -167,8 +178,13 @@@
          // the values to add microseconds precision.
          if (column.hasDictionary() || (rowId == 0 &&
              (descriptor.getType() == PrimitiveType.PrimitiveTypeName.INT32 ||
++<<<<<<< HEAD
 +            (descriptor.getType() == PrimitiveType.PrimitiveTypeName.INT64 &&
 +               column.dataType() != DataTypes.TimestampType) ||
++=======
+             (descriptor.getType() == PrimitiveType.PrimitiveTypeName.INT64  &&
+               originalType != OriginalType.TIMESTAMP_MILLIS) ||
++>>>>>>> origin/master
              descriptor.getType() == PrimitiveType.PrimitiveTypeName.FLOAT ||
              descriptor.getType() == PrimitiveType.PrimitiveTypeName.DOUBLE ||
              descriptor.getType() == PrimitiveType.PrimitiveTypeName.BINARY))) {
@@@ -259,16 -258,14 +274,25 @@@
  
        case INT64:
          if (column.dataType() == DataTypes.LongType ||
++<<<<<<< HEAD
 +                (column.dataType() == DataTypes.TimestampType
 +                        && fullType.getOriginalType() == OriginalType.TIMESTAMP_MICROS) ||
 +            DecimalType.is64BitDecimalType(column.dataType())) {
++=======
+             DecimalType.is64BitDecimalType(column.dataType()) ||
+             originalType == OriginalType.TIMESTAMP_MICROS) {
++>>>>>>> origin/master
            for (int i = rowId; i < rowId + num; ++i) {
              if (!column.isNullAt(i)) {
                column.putLong(i, dictionary.decodeToLong(dictionaryIds.getDictId(i)));
              }
            }
++<<<<<<< HEAD
 +        } else if (column.dataType() == DataTypes.TimestampType &&
 +                fullType.getOriginalType() == OriginalType.TIMESTAMP_MILLIS) {
++=======
+         } else if (originalType == OriginalType.TIMESTAMP_MILLIS) {
++>>>>>>> origin/master
            for (int i = rowId; i < rowId + num; ++i) {
              if (!column.isNullAt(i)) {
                column.putLong(i,
@@@ -386,13 -382,11 +409,21 @@@
    private void readLongBatch(int rowId, int num, WritableColumnVector column) {
      // This is where we implement support for the valid type conversions.
      if (column.dataType() == DataTypes.LongType ||
++<<<<<<< HEAD
 +            (column.dataType() == DataTypes.TimestampType &&
 +            fullType.getOriginalType() == OriginalType.TIMESTAMP_MICROS)||
 +        DecimalType.is64BitDecimalType(column.dataType())) {
 +      defColumn.readLongs(
 +        num, column, rowId, maxDefLevel, (VectorizedValuesReader) dataColumn);
 +    } else if (column.dataType() == DataTypes.TimestampType &&
 +            fullType.getOriginalType() == OriginalType.TIMESTAMP_MILLIS) {
++=======
+         DecimalType.is64BitDecimalType(column.dataType()) ||
+         originalType == OriginalType.TIMESTAMP_MICROS) {
+       defColumn.readLongs(
+         num, column, rowId, maxDefLevel, (VectorizedValuesReader) dataColumn);
+     } else if (originalType == OriginalType.TIMESTAMP_MILLIS) {
++>>>>>>> origin/master
        for (int i = 0; i < num; i++) {
          if (defColumn.readInteger() == maxDefLevel) {
            column.putLong(rowId + i, DateTimeUtils.fromMillis(dataColumn.readLong()));
diff --cc sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java
index 248c92f1eb,e827229dce..0000000000
--- a/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java
+++ b/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java
@@@ -284,8 -287,8 +287,13 @@@ public class VectorizedParquetRecordRea
      columnReaders = new VectorizedColumnReader[columns.size()];
      for (int i = 0; i < columns.size(); ++i) {
        if (missingColumns[i]) continue;
++<<<<<<< HEAD
 +      columnReaders[i] = new VectorizedColumnReader(columns.get(i),
 +          pages.getPageReader(columns.get(i)), requestedSchema.getType(i));
++=======
+       columnReaders[i] = new VectorizedColumnReader(
+         columns.get(i), types.get(i).getOriginalType(), pages.getPageReader(columns.get(i)));
++>>>>>>> origin/master
      }
      totalCountLoadedSoFar += pages.getRowCount();
    }
diff --cc sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala
index 78c77ca4ee,044b1a89d5..0000000000
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala
@@@ -133,12 -118,8 +125,17 @@@ class ParquetFileForma
        sparkSession.sessionState.conf.writeLegacyParquetFormat.toString)
  
      conf.set(
++<<<<<<< HEAD
 +      SQLConf.PARQUET_TIMESTAMP_AS_INT96.key,
 +      sparkSession.sessionState.conf.isParquetTimestampAsINT96.toString)
 +
 +    conf.set(
 +      SQLConf.PARQUET_INT64_AS_TIMESTAMP_MILLIS.key,
 +      sparkSession.sessionState.conf.isParquetINT64AsTimestampMillis.toString)
++=======
+       SQLConf.PARQUET_OUTPUT_TIMESTAMP_TYPE.key,
+       sparkSession.sessionState.conf.parquetOutputTimestampType.toString)
++>>>>>>> origin/master
  
      // Sets compression scheme
      conf.set(ParquetOutputFormat.COMPRESSION, parquetOptions.compressionCodecClassName)
@@@ -386,21 -304,13 +383,31 @@@
  
      ParquetWriteSupport.setSchema(requiredSchema, hadoopConf)
  
++<<<<<<< HEAD
 +    val int96AsTimestamp = sparkSession.sessionState.conf.isParquetINT96AsTimestamp
 +    // Sets flags for `CatalystSchemaConverter`
 +    hadoopConf.setBoolean(
 +      SQLConf.PARQUET_BINARY_AS_STRING.key,
 +      sparkSession.sessionState.conf.isParquetBinaryAsString)
 +    hadoopConf.setBoolean(SQLConf.PARQUET_INT96_AS_TIMESTAMP.key, int96AsTimestamp)
 +    hadoopConf.setBoolean(
 +      SQLConf.PARQUET_INT64_AS_TIMESTAMP_MILLIS.key,
 +      sparkSession.sessionState.conf.isParquetINT64AsTimestampMillis)
++=======
+     // Sets flags for `ParquetToSparkSchemaConverter`
+     hadoopConf.setBoolean(
+       SQLConf.PARQUET_BINARY_AS_STRING.key,
+       sparkSession.sessionState.conf.isParquetBinaryAsString)
+     hadoopConf.setBoolean(
+       SQLConf.PARQUET_INT96_AS_TIMESTAMP.key,
+       sparkSession.sessionState.conf.isParquetINT96AsTimestamp)
++>>>>>>> origin/master
 +
 +    // By default, disable record level filtering.
 +    if (hadoopConf.get(ParquetInputFormat.RECORD_FILTERING_ENABLED) == null) {
 +      hadoopConf.setBoolean(ParquetInputFormat.RECORD_FILTERING_ENABLED, false)
 +    }
 +
  
      // Try to push down filters when filter push-down is enabled.
      val pushed =
@@@ -523,16 -417,9 +530,22 @@@ object ParquetFileFormat extends Loggin
    private[parquet] def readSchema(
        footers: Seq[Footer], sparkSession: SparkSession): Option[StructType] = {
  
++<<<<<<< HEAD
 +    def parseParquetSchema(schema: MessageType): StructType = {
 +      val converter = new ParquetSchemaConverter(
 +        sparkSession.sessionState.conf.isParquetBinaryAsString,
 +        sparkSession.sessionState.conf.isParquetINT96AsTimestamp,
 +        sparkSession.sessionState.conf.writeLegacyParquetFormat,
 +        sparkSession.sessionState.conf.isParquetTimestampAsINT96,
 +        sparkSession.sessionState.conf.isParquetINT64AsTimestampMillis)
 +
 +      converter.convert(schema)
 +    }
++=======
+     val converter = new ParquetToSparkSchemaConverter(
+       sparkSession.sessionState.conf.isParquetBinaryAsString,
+       sparkSession.sessionState.conf.isParquetINT96AsTimestamp)
++>>>>>>> origin/master
  
      val seen = mutable.HashSet[String]()
      val finalSchemas: Seq[StructType] = footers.flatMap { footer =>
@@@ -636,9 -521,6 +649,12 @@@
        sparkSession: SparkSession): Option[StructType] = {
      val assumeBinaryIsString = sparkSession.sessionState.conf.isParquetBinaryAsString
      val assumeInt96IsTimestamp = sparkSession.sessionState.conf.isParquetINT96AsTimestamp
++<<<<<<< HEAD
 +    val writeTimestampInMillis = sparkSession.sessionState.conf.isParquetINT64AsTimestampMillis
 +    val writeLegacyParquetFormat = sparkSession.sessionState.conf.writeLegacyParquetFormat
 +    val writeTimestampAsInt96 = sparkSession.sessionState.conf.isParquetTimestampAsINT96
++=======
++>>>>>>> origin/master
      val serializedConf = new SerializableConfiguration(sparkSession.sessionState.newHadoopConf())
  
      // !! HACK ALERT !!
@@@ -678,14 -560,9 +694,20 @@@
                serializedConf.value, fakeFileStatuses, ignoreCorruptFiles)
  
            // Converter used to convert Parquet `MessageType` to Spark SQL `StructType`
++<<<<<<< HEAD
 +          val converter =
 +            new ParquetSchemaConverter(
 +              assumeBinaryIsString = assumeBinaryIsString,
 +              assumeInt96IsTimestamp = assumeInt96IsTimestamp,
 +              writeLegacyParquetFormat = writeLegacyParquetFormat,
 +              writeTimestampAsInt96 = writeTimestampAsInt96,
 +              writeTimestampInMillis = writeTimestampInMillis)
 +
++=======
+           val converter = new ParquetToSparkSchemaConverter(
+             assumeBinaryIsString = assumeBinaryIsString,
+             assumeInt96IsTimestamp = assumeInt96IsTimestamp)
++>>>>>>> origin/master
            if (footers.isEmpty) {
              Iterator.empty
            } else {
diff --cc sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala
index 8c6a21de05,10f6c3b4f1..0000000000
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala
@@@ -253,7 -253,11 +253,15 @@@ private[parquet] class ParquetRowConver
          new ParquetStringConverter(updater)
  
        case TimestampType if parquetType.getOriginalType == OriginalType.TIMESTAMP_MICROS =>
++<<<<<<< HEAD
 +        new ParquetPrimitiveConverter(updater)
++=======
+         new ParquetPrimitiveConverter(updater) {
+           override def addLong(value: Long): Unit = {
+             updater.setLong(value)
+           }
+         }
++>>>>>>> origin/master
  
        case TimestampType if parquetType.getOriginalType == OriginalType.TIMESTAMP_MILLIS =>
          new ParquetPrimitiveConverter(updater) {
@@@ -262,7 -266,8 +270,12 @@@
            }
          }
  
++<<<<<<< HEAD
 +      case TimestampType =>
++=======
+       // INT96 timestamp doesn't have a logical type, here we check the physical type instead.
+       case TimestampType if parquetType.asPrimitiveType().getPrimitiveTypeName == INT96 =>
++>>>>>>> origin/master
          new ParquetPrimitiveConverter(updater) {
            // Converts nanosecond timestamps stored as INT96
            override def addBinary(value: Binary): Unit = {
diff --cc sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala
index 46b1b41733,c61be077d3..0000000000
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala
@@@ -38,47 -38,24 +38,63 @@@ import org.apache.spark.sql.types.
   * [[MessageType]] schemas.
   *
   * @see https://github.com/apache/parquet-format/blob/master/LogicalTypes.md
-  * @constructor
+  *
   * @param assumeBinaryIsString Whether unannotated BINARY fields should be assumed to be Spark SQL
-  *        [[StringType]] fields when converting Parquet a [[MessageType]] to Spark SQL
-  *        [[StructType]].  This argument only affects Parquet read path.
+  *        [[StringType]] fields.
   * @param assumeInt96IsTimestamp Whether unannotated INT96 fields should be assumed to be Spark SQL
++<<<<<<< HEAD
 + *        [[TimestampType]] fields when converting Parquet a [[MessageType]] to Spark SQL
 + *        [[StructType]].  Note that Spark SQL [[TimestampType]] is similar to Hive timestamp, which
 + *        has optional nanosecond precision, but different from `TIME_MILLS` and `TIMESTAMP_MILLIS`
 + *        described in Parquet format spec.  This argument only affects Parquet read path.
 + * @param writeLegacyParquetFormat Whether to use legacy Parquet format compatible with Spark 1.4
 + *        and prior versions when converting a Catalyst [[StructType]] to a Parquet [[MessageType]].
 + *        When set to false, use standard format defined in parquet-format spec.  This argument only
 + *        affects Parquet write path.
 + * @param writeTimestampAsInt96 Whether to use serialize Timestamps in legacy INT96 format for
 + *        forwards compatibility.
 + * @param writeTimestampInMillis Whether to write timestamp values as INT64 annotated by logical
 + *        type TIMESTAMP_MILLIS.
++=======
+  *        [[TimestampType]] fields.
++>>>>>>> origin/master
   */
- private[parquet] class ParquetSchemaConverter(
+ class ParquetToSparkSchemaConverter(
      assumeBinaryIsString: Boolean = SQLConf.PARQUET_BINARY_AS_STRING.defaultValue.get,
++<<<<<<< HEAD
 +    assumeInt96IsTimestamp: Boolean = SQLConf.PARQUET_INT96_AS_TIMESTAMP.defaultValue.get,
 +    writeLegacyParquetFormat: Boolean = SQLConf.PARQUET_WRITE_LEGACY_FORMAT.defaultValue.get,
 +    writeTimestampAsInt96: Boolean = SQLConf.PARQUET_TIMESTAMP_AS_INT96.defaultValue.get,
 +    writeTimestampInMillis: Boolean = SQLConf.PARQUET_INT64_AS_TIMESTAMP_MILLIS.defaultValue.get) {
 +
 +  def this(conf: SQLConf) = this(
 +    assumeBinaryIsString = conf.isParquetBinaryAsString,
 +    assumeInt96IsTimestamp = conf.isParquetINT96AsTimestamp,
 +    writeLegacyParquetFormat = conf.writeLegacyParquetFormat,
 +    writeTimestampAsInt96 = conf.isParquetTimestampAsINT96,
 +    writeTimestampInMillis = conf.isParquetINT64AsTimestampMillis)
 +
 +  def this(conf: Configuration) = this(
 +    assumeBinaryIsString = conf.get(SQLConf.PARQUET_BINARY_AS_STRING.key).toBoolean,
 +    assumeInt96IsTimestamp = conf.get(SQLConf.PARQUET_INT96_AS_TIMESTAMP.key).toBoolean,
 +    writeLegacyParquetFormat = conf.get(SQLConf.PARQUET_WRITE_LEGACY_FORMAT.key,
 +      SQLConf.PARQUET_WRITE_LEGACY_FORMAT.defaultValue.get.toString).toBoolean,
 +    writeTimestampAsInt96 = conf.get(SQLConf.PARQUET_TIMESTAMP_AS_INT96.key,
 +      SQLConf.PARQUET_TIMESTAMP_AS_INT96.defaultValue.get.toString).toBoolean,
 +    writeTimestampInMillis = conf.get(SQLConf.PARQUET_INT64_AS_TIMESTAMP_MILLIS.key,
 +      SQLConf.PARQUET_INT64_AS_TIMESTAMP_MILLIS.defaultValue.get.toString).toBoolean)
++=======
+     assumeInt96IsTimestamp: Boolean = SQLConf.PARQUET_INT96_AS_TIMESTAMP.defaultValue.get) {
+ 
+   def this(conf: SQLConf) = this(
+     assumeBinaryIsString = conf.isParquetBinaryAsString,
+     assumeInt96IsTimestamp = conf.isParquetINT96AsTimestamp)
+ 
+   def this(conf: Configuration) = this(
+     assumeBinaryIsString = conf.get(SQLConf.PARQUET_BINARY_AS_STRING.key).toBoolean,
+     assumeInt96IsTimestamp = conf.get(SQLConf.PARQUET_INT96_AS_TIMESTAMP.key).toBoolean)
+ 
++>>>>>>> origin/master
  
    /**
     * Converts Parquet [[MessageType]] `parquetSchema` to a Spark SQL [[StructType]].
@@@ -378,17 -381,18 +420,32 @@@ class SparkToParquetSchemaConverter
        // `TIMESTAMP_MICROS` which are both logical types annotating `INT64`.
        //
        // Originally, Spark SQL uses the same nanosecond timestamp type as Impala and Hive.  Starting
++<<<<<<< HEAD
 +      // from Spark 1.5.0, we resort to a timestamp type with 100 ns precision so that we can store
 +      // a timestamp into a `Long`.  This design decision is subject to change though, for example,
 +      // we may resort to microsecond precision in the future.
 +      case TimestampType if writeTimestampInMillis =>
 +        Types.primitive(INT64, repetition).as(TIMESTAMP_MILLIS).named(field.name)
 +
 +      case TimestampType if writeTimestampAsInt96 =>
 +        Types.primitive(INT96, repetition).named(field.name)
++=======
+       // from Spark 1.5.0, we resort to a timestamp type with microsecond precision so that we can
+       // store a timestamp into a `Long`.  This design decision is subject to change though, for
+       // example, we may resort to nanosecond precision in the future.
+       case TimestampType =>
+         outputTimestampType match {
+           case SQLConf.ParquetOutputTimestampType.INT96 =>
+             Types.primitive(INT96, repetition).named(field.name)
+           case SQLConf.ParquetOutputTimestampType.TIMESTAMP_MICROS =>
+             Types.primitive(INT64, repetition).as(TIMESTAMP_MICROS).named(field.name)
+           case SQLConf.ParquetOutputTimestampType.TIMESTAMP_MILLIS =>
+             Types.primitive(INT64, repetition).as(TIMESTAMP_MILLIS).named(field.name)
+         }
++>>>>>>> origin/master
 +
 +      case TimestampType =>
 +        Types.primitive(INT64, repetition).as(TIMESTAMP_MICROS).named(field.name)
  
        case BinaryType =>
          Types.primitive(BINARY, repetition).named(field.name)
diff --cc sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala
index 7c7ab8e5f8,af4e1433c8..0000000000
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala
@@@ -69,11 -66,11 +69,16 @@@ private[parquet] class ParquetWriteSupp
    // Whether to write data in legacy Parquet format compatible with Spark 1.4 and prior versions
    private var writeLegacyParquetFormat: Boolean = _
  
++<<<<<<< HEAD
 +  // Whether to write timestamps as int96
 +  private var writeTimestampAsInt96: Boolean = _
 +
 +  // Whether to write timestamp value with milliseconds precision.
 +  private var writeTimestampInMillis: Boolean = _
++=======
+   // Which parquet timestamp type to use when writing.
+   private var outputTimestampType: SQLConf.ParquetOutputTimestampType.Value = _
 -
 -  // Reusable byte array used to write timestamps as Parquet INT96 values
 -  private val timestampBuffer = new Array[Byte](12)
++>>>>>>> origin/master
  
    // Reusable byte array used to write decimal values
    private val decimalBuffer = new Array[Byte](minBytesForPrecision(DecimalType.MAX_PRECISION))
@@@ -86,14 -83,11 +91,15 @@@
        assert(configuration.get(SQLConf.PARQUET_WRITE_LEGACY_FORMAT.key) != null)
        configuration.get(SQLConf.PARQUET_WRITE_LEGACY_FORMAT.key).toBoolean
      }
 +    this.writeTimestampAsInt96 = {
 +      assert(configuration.get(SQLConf.PARQUET_TIMESTAMP_AS_INT96.key) != null)
 +      configuration.get(SQLConf.PARQUET_TIMESTAMP_AS_INT96.key).toBoolean
 +    }
  
-     this.writeTimestampInMillis = {
-       assert(configuration.get(SQLConf.PARQUET_INT64_AS_TIMESTAMP_MILLIS.key) != null)
-       configuration.get(SQLConf.PARQUET_INT64_AS_TIMESTAMP_MILLIS.key).toBoolean
+     this.outputTimestampType = {
+       val key = SQLConf.PARQUET_OUTPUT_TIMESTAMP_TYPE.key
+       assert(configuration.get(key) != null)
+       SQLConf.ParquetOutputTimestampType.withName(configuration.get(key))
      }
  
      this.rootFieldWriters = schema.map(_.dataType).map(makeWriter).toArray[ValueWriter]
@@@ -169,7 -163,24 +175,28 @@@
            recordConsumer.addBinary(
              Binary.fromReusedByteArray(row.getUTF8String(ordinal).getBytes))
  
++<<<<<<< HEAD
 +      case TimestampType => makeTimestampWriter()
++=======
+       case TimestampType =>
+         outputTimestampType match {
+           case SQLConf.ParquetOutputTimestampType.INT96 =>
+             (row: SpecializedGetters, ordinal: Int) =>
+               val (julianDay, timeOfDayNanos) = DateTimeUtils.toJulianDay(row.getLong(ordinal))
+               val buf = ByteBuffer.wrap(timestampBuffer)
+               buf.order(ByteOrder.LITTLE_ENDIAN).putLong(timeOfDayNanos).putInt(julianDay)
+               recordConsumer.addBinary(Binary.fromReusedByteArray(timestampBuffer))
+ 
+           case SQLConf.ParquetOutputTimestampType.TIMESTAMP_MICROS =>
+             (row: SpecializedGetters, ordinal: Int) =>
+               recordConsumer.addLong(row.getLong(ordinal))
+ 
+           case SQLConf.ParquetOutputTimestampType.TIMESTAMP_MILLIS =>
+             (row: SpecializedGetters, ordinal: Int) =>
+               val millis = DateTimeUtils.toMillis(row.getLong(ordinal))
+               recordConsumer.addLong(millis)
+         }
++>>>>>>> origin/master
  
        case BinaryType =>
          (row: SpecializedGetters, ordinal: Int) =>
